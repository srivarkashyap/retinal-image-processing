{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzwjlOp00bCx1dNKjEA++Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mini-project4ppl/Major_Project/blob/main/DR_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to input and output directories\n",
        "# input_dir = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\diabetic-retinopathy-unziped\\\\main train\\\\main train' #dataset path\n",
        "input_dir= \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\DR\" #path of dataset containg subfolders"
      ],
      "metadata": {
        "id": "nysUPCROdYB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CLAHE parameters\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "c=0\n",
        "\n",
        "# Loop through each class directory in the input directory\n",
        "for class_dir in os.listdir(input_dir):\n",
        "    input_class_path = os.path.join(input_dir, class_dir)\n",
        "\n",
        "    # Loop through each image in the class directory\n",
        "    for img_name in os.listdir(input_class_path):\n",
        "\n",
        "        # Read image\n",
        "        img_path = os.path.join(input_class_path, img_name)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Apply CLAHE\n",
        "        img_clahe = clahe.apply(img)\n",
        "\n",
        "        # Save CLAHE enhanced image, replacing the existing file\n",
        "        img_path_clahe = img_path  # Use the same file path\n",
        "        cv2.imwrite(img_path_clahe, img_clahe)\n",
        "\n",
        "    print(\"Level\",c,\"done\")\n",
        "    c=c+1\n",
        "\n",
        "print(\"Preprocessing complete.\")\n"
      ],
      "metadata": {
        "id": "9tVzCqRudgna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DCGAN\n",
        "\n",
        "#GAN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def weights_init(w):\n",
        "    \"\"\"\n",
        "    Initializes the weights of the layer, w.\n",
        "    \"\"\"\n",
        "    classname = w.__class__.__name__\n",
        "    if classname.find('conv') != -1:\n",
        "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('bn') != -1:\n",
        "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(w.bias.data, 0)\n",
        "\n",
        "# Define the Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input is the latent vector Z.\n",
        "        self.tconv1 = nn.ConvTranspose2d(params['nz'], params['ngf']*8*2,\n",
        "            kernel_size=5, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(params['ngf']*8*2)\n",
        "\n",
        "        # Input Dimension: (ngf*8) x 5 x 5\n",
        "        self.tconv2 = nn.ConvTranspose2d(params['ngf']*8*2, params['ngf']*4*2,\n",
        "            5, 3, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(params['ngf']*4*2)\n",
        "\n",
        "        # Input Dimension: (ngf*4) x 15 x 15\n",
        "        self.tconv3 = nn.ConvTranspose2d(params['ngf']*4*2, params['ngf']*2*2,\n",
        "            5, 3, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(params['ngf']*2*2)\n",
        "\n",
        "        # Input Dimension: (ngf*2) x 45 x 45\n",
        "        self.tconv4 = nn.ConvTranspose2d(params['ngf']*2*2, params['ngf']*2,\n",
        "            4, 2, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(params['ngf']*2)\n",
        "\n",
        "        # Input Dimension: (ngf*2) x 90 x 90\n",
        "        self.tconv5 = nn.ConvTranspose2d(params['ngf']*2, params['ngf'],\n",
        "            4, 2, 1, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(params['ngf'])\n",
        "\n",
        "        # # Input Dimension: (ngf*2) x 180 x 180\n",
        "        # self.tconv6 = nn.ConvTranspose2d(params['ngf']*2, params['ngf'],\n",
        "        #     4, 2, 1, bias=False)\n",
        "        # self.bn6 = nn.BatchNorm2d(params['ngf'])\n",
        "\n",
        "        # Input Dimension: (ngf*2) x 180 x 180\n",
        "        self.tconv7 = nn.ConvTranspose2d(params['ngf'], params['nc'],\n",
        "            4, 2, 1, bias=False)\n",
        "        #Output Dimension: (nc) 360 x 360\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.tconv1(x)))\n",
        "        x = F.relu(self.bn2(self.tconv2(x)))\n",
        "        x = F.relu(self.bn3(self.tconv3(x)))\n",
        "        x = F.relu(self.bn4(self.tconv4(x)))\n",
        "        x = F.relu(self.bn5(self.tconv5(x)))\n",
        "        # x = F.relu(self.bn6(self.tconv6(x)))\n",
        "\n",
        "\n",
        "        x = F.tanh(self.tconv7(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define the Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input Dimension: (nc) x 360 x 360\n",
        "        self.conv1 = nn.Conv2d(params['nc'], params['ndf'],\n",
        "            4, 2, 1, bias=False)\n",
        "\n",
        "        # Input Dimension: (ndf) x 180 x 180\n",
        "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
        "            4, 2, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
        "\n",
        "        # Input Dimension: (ndf*2) x 90 x 90\n",
        "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
        "            4, 2, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
        "\n",
        "\n",
        "        # Input Dimension: (ndf*8) x 45 x 45\n",
        "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8,\n",
        "            5, 3, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
        "        # Output Dimesnion 15 x 15\n",
        "\n",
        "        # Input Dimension: (ndf*8) x 15 x 15\n",
        "        self.conv5 = nn.Conv2d(params['ndf']*8, params['ndf']*16,\n",
        "            5, 3, 1, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(params['ndf']*16)\n",
        "        # Output Dimesnion 5 x 5\n",
        "\n",
        "\n",
        "        # Input 5 x 5\n",
        "        self.conv6 = nn.Conv2d(params['ndf']*16, 1, 5, 1, 0, bias=False)\n",
        "        # OUtput 1 x 1\n",
        "        # self.bn5 = nn.BatchNorm2d(params['ndf']*1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
        "        x = F.leaky_relu(self.bn5(self.conv5(x)), 0.2, True)\n",
        "\n",
        "        x = F.sigmoid(self.conv6(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "# Directory containing the data on your local machine.\n",
        "root = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\DR\"\n",
        "\n",
        "def get_dataset(params):\n",
        "    \"\"\"\n",
        "    Loads the dataset and applies preprocessing steps to it.\n",
        "    Returns a PyTorch DataLoader.\n",
        "\n",
        "    \"\"\"\n",
        "    # Data preprocessing.\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(params['imsize']),\n",
        "        transforms.CenterCrop(params['imsize']),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # Create the dataset.\n",
        "    dataset = dset.ImageFolder(root=root, transform=transform)\n",
        "\n",
        "    # Create the dataloader.\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=params['bsize'],\n",
        "                                             shuffle=True,\n",
        "                                             num_workers=0)  # Change this to the number of CPU cores available on your local machine, or keep it as 0 if you don't want to use multiprocessing.\n",
        "\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "myxb_Xx8dxoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DCGAN Train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Directory where you want to save the model checkpoints.\n",
        "model_dir = 'C:\\\\Users\\\\akura\\\\OneDrive\\\\Documents\\\\dcgan_model.pth'\n",
        "\n",
        "# Set random seed for reproducibility.\n",
        "seed = 369\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "print(\"Random Seed: \", seed)\n",
        "\n",
        "# Parameters to define the model.\n",
        "params = {\n",
        "    \"bsize\": 2,  # Batch size during training.\n",
        "    'imsize': 360,  # Spatial size of training images. All images will be resized to this size during preprocessing.\n",
        "    'nc': 3,  # Number of channels in the training images. For colored images, this is 3.\n",
        "    'nz': 100,  # Size of the Z latent vector (the input to the generator).\n",
        "    'ngf': 32,  # Size of feature maps in the generator. The depth will be multiples of this.\n",
        "    'ndf': 64,  # Size of feature maps in the discriminator. The depth will be multiples of this.\n",
        "    'nepochs': 10,  # Number of training epochs.\n",
        "    'lr': 0.0002,  # Learning rate for optimizers.\n",
        "    'beta1': 0.5,  # Beta1 hyperparameter for Adam optimizer.\n",
        "    'save_epoch': 2  # Save step.\n",
        "}\n",
        "\n",
        "# Use GPU if available, else use CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device, \" will be used.\\n\")\n",
        "\n",
        "# Get the data.\n",
        "dataloader = get_dataset(params)\n",
        "\n",
        "# Create the generator.\n",
        "netG = Generator(params).to(device)\n",
        "netG.apply(weights_init)\n",
        "print(netG)\n",
        "\n",
        "# Create the discriminator.\n",
        "netD = Discriminator(params).to(device)\n",
        "netD.apply(weights_init)\n",
        "netD.add_module('sigmoid', nn.Sigmoid())\n",
        "print(netD)\n",
        "\n",
        "# Binary Cross Entropy loss function.\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Optimizer for the discriminator.\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
        "# Optimizer for the generator.\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
        "\n",
        "# Stores generated images as training progresses.\n",
        "img_list = []\n",
        "# Stores generator losses during training.\n",
        "G_losses = []\n",
        "# Stores discriminator losses during training.\n",
        "D_losses = []\n",
        "\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "for epoch in range(params['nepochs']):\n",
        "    batches_done = 0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        batches_done += 1\n",
        "        real_data = data[0].to(device)\n",
        "        b_size = real_data.size(0)\n",
        "\n",
        "        netD.zero_grad()\n",
        "        label = torch.full((b_size,), real_label, device=device).float()  # Cast to float\n",
        "        output = netD(real_data).view(-1)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
        "        fake_data = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake_data.detach()).view(-1)\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "        output = netD(fake_data).view(-1)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, params['nepochs'], i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        if (iters % 100 == 0) or ((epoch == params['nepochs'] - 1) and (i == len(dataloader) - 1)):\n",
        "            with torch.no_grad():\n",
        "                fake_data = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1\n",
        "        print(\"Epoch : {}, Batch : {}\".format(epoch, batches_done))\n",
        "\n",
        "    if epoch % params['save_epoch'] == 0:\n",
        "        torch.save({\n",
        "            'generator': netG.state_dict(),\n",
        "            'discriminator': netD.state_dict(),\n",
        "            'optimizerG': optimizerG.state_dict(),\n",
        "            'optimizerD': optimizerD.state_dict(),\n",
        "            'params': params\n",
        "        }, 'model_epoch_{}.pth'.format(epoch))\n",
        "\n",
        "torch.save({\n",
        "    'generator': netG.state_dict(),\n",
        "    'discriminator': netD.state_dict(),\n",
        "    'optimizerG': optimizerG.state_dict(),\n",
        "    'optimizerD': optimizerD.state_dict(),\n",
        "    'params': params\n",
        "}, 'model_final.pth')\n"
      ],
      "metadata": {
        "id": "dK5GTiIWf68b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DCGAN Generate\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the checkpoint file.\n",
        "path = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\model_final.pth\"\n",
        "state_dict = torch.load(path, map_location='cpu')\n",
        "\n",
        "# Set the device to run on: GPU or CPU.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Get the 'params' dictionary from the loaded state_dict.\n",
        "params = state_dict['params']\n",
        "\n",
        "# Create the generator network.\n",
        "netG = Generator(params).to(device)\n",
        "# Load the trained generator weights.\n",
        "netG.load_state_dict(state_dict['generator'])\n",
        "print(netG)\n",
        "\n",
        "# Set the number of generated outputs\n",
        "num_output = 1000\n",
        "\n",
        "# Path to the test dataset with 5 subfolders.\n",
        "test_dataset_path = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Test\"\n",
        "\n",
        "# Create a new dataset with 5 subfolders for generated images.\n",
        "output_dataset_path = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\GAN\"\n",
        "os.makedirs(output_dataset_path, exist_ok=True)\n",
        "\n",
        "# Generate images for each subfolder in the test dataset.\n",
        "for subfolder_name in os.listdir(test_dataset_path):\n",
        "    subfolder_path = os.path.join(test_dataset_path, subfolder_name)\n",
        "\n",
        "    # Create subfolder in the generated dataset.\n",
        "    output_subfolder_path = os.path.join(output_dataset_path, subfolder_name)\n",
        "    os.makedirs(output_subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Get latent vector Z from unit normal distribution.\n",
        "    noise = torch.randn(num_output, params['nz'], 1, 1, device=device)\n",
        "\n",
        "    # Turn off gradient calculation to speed up the process.\n",
        "    with torch.no_grad():\n",
        "        # Get generated image from the noise vector using the trained generator.\n",
        "        generated_img = netG(noise).detach().cpu()\n",
        "\n",
        "    # Save generated images to the corresponding subfolder.\n",
        "    for i in range(num_output):\n",
        "        img_filename = f\"pic_generated_{subfolder_name}_{i}.jpg\"\n",
        "        img_path = os.path.join(output_subfolder_path, img_filename)\n",
        "\n",
        "        # Convert PyTorch tensor to NumPy array and normalize values\n",
        "        img_np = np.transpose(vutils.make_grid(generated_img[i], padding=2, normalize=True).numpy(), (1, 2, 0))\n",
        "        plt.imsave(img_path, img_np)\n",
        "\n",
        "print(\"Generated images saved to:\", output_dataset_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "4WPxVNedd2n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training DenseNet-201 model\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.models as models\n",
        "from torch.nn.parallel import DataParallel\n",
        "\n",
        "# Set the path to your dataset\n",
        "data_dir = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\DR'\n",
        "# Define the transformations applied to the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ImageFolder(data_dir, transform=transform)\n",
        "\n",
        "# Define the DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Load the pre-trained DenseNet201 model\n",
        "model = models.densenet201(pretrained=True)\n",
        "\n",
        "# Replace the classifier to match the number of classes in your dataset\n",
        "num_classes = len(dataset.classes)\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Check if multiple GPUs are available and move the model to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for data parallelism.\")\n",
        "    model = DataParallel(model)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Save the trained model to your desired location\n",
        "save_dir = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\"\n",
        "model_name = \"Densenet.pth\"\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "ImTiWI8TeKs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train AlexNet\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Set the paths for your local system\n",
        "train_data_dir = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\DR'  # Adjust this path\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "input_shape = (224, 224, 3)  # Adjust the input size based on your dataset\n",
        "num_classes = 5  # Number of severity classes\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(384, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(384, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# No validation_generator and validation_data in the fit method\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[ModelCheckpoint('fundus_severity_model.h5', monitor='accuracy', save_best_only=True, mode='max', verbose=1),\n",
        "               EarlyStopping(monitor='accuracy', patience=10, mode='max', verbose=1)])\n",
        "\n",
        "# Save the trained model\n",
        "model.save('Alexnet.h5')\n",
        "\n",
        "import os\n",
        "\n",
        "# Save the trained model to your desired location\n",
        "save_dir = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\"\n",
        "model_name = \"Alexnet_trail.h5\"\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)"
      ],
      "metadata": {
        "id": "C0YD_h_Rd5ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert .h5 to .pth\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "keras_model = load_model('C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Alexnet_trail.h5')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define your PyTorch model class\n",
        "class PyTorchModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PyTorchModel, self).__init__()\n",
        "        # Define your layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of your model\n",
        "        return x\n",
        "\n",
        "# Create an instance of your PyTorch model\n",
        "pytorch_model = PyTorchModel()\n",
        "\n",
        "\n",
        "# Iterate over layers and transfer weights\n",
        "for keras_layer, pytorch_layer in zip(keras_model.layers, pytorch_model.modules()):\n",
        "    if isinstance(pytorch_layer, nn.Conv2d) or isinstance(pytorch_layer, nn.Linear):\n",
        "        # Transfer weights\n",
        "        pytorch_layer.weight.data = torch.tensor(keras_layer.get_weights()[0])\n",
        "        pytorch_layer.bias.data = torch.tensor(keras_layer.get_weights()[1])\n",
        "\n",
        "\n",
        "torch.save(pytorch_model.state_dict(), 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Alexnet_trail.pth')"
      ],
      "metadata": {
        "id": "9jlf4rsPd95I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Densenet accuracy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "# Set the path to the folder containing test images\n",
        "test_data_dir = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Test'\n",
        "\n",
        "# Define the transformations applied to the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = ImageFolder(test_data_dir, transform=transform)\n",
        "\n",
        "# Create the DataLoader for the test data\n",
        "batch_size = 64\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load the pre-trained DenseNet model\n",
        "dn_model_path = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Densenet.pth\"\n",
        "dn_model = models.densenet201(pretrained=True)\n",
        "dn_model.classifier = nn.Linear(dn_model.classifier.in_features, len(test_dataset.classes))\n",
        "dn_model.load_state_dict(torch.load(dn_model_path), strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dn_model.to(device)\n",
        "dn_model.eval()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the DenseNet model on the test dataset\n",
        "dn_model_accuracy = evaluate_model(dn_model, test_dataloader)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy for DenseNet Model: {dn_model_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ZNosCCyTeOH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Alex Net Accuracy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "# Set the path to the folder containing test images\n",
        "test_data_dir = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Test'\n",
        "\n",
        "# Define the transformations applied to the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = ImageFolder(test_data_dir, transform=transform)\n",
        "\n",
        "# Create the DataLoader for the test data\n",
        "batch_size = 16\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load the pre-trained AlexNet model\n",
        "alexnet_model_path = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Alex_final.pth'\n",
        "alexnet_model = models.alexnet(pretrained=True)\n",
        "alexnet_model.classifier[6] = nn.Linear(4096, len(test_dataset.classes))\n",
        "alexnet_model.load_state_dict(torch.load(alexnet_model_path), strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet_model.to(device)\n",
        "alexnet_model.eval()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the AlexNet model on the test dataset\n",
        "alexnet_model_accuracy = evaluate_model(alexnet_model, test_dataloader)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy for AlexNet Model: {alexnet_model_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "LBuZhzJVeO8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the path to the folder containing test images\n",
        "test_data_dir = 'C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Test'\n",
        "\n",
        "# Define the transformations applied to the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = ImageFolder(test_data_dir, transform=transform)\n",
        "\n",
        "# Create the DataLoader for the test data\n",
        "batch_size = 64\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load the pre-trained AlexNet model\n",
        "alexnet_model_path = (\"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Alex_final.pth\")\n",
        "alexnet_model = models.alexnet(pretrained=True)\n",
        "alexnet_model.classifier[6] = nn.Linear(4096, len(test_dataset.classes))\n",
        "alexnet_model.load_state_dict(torch.load(alexnet_model_path), strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet_model.to(device)\n",
        "alexnet_model.eval()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the AlexNet model on the test dataset\n",
        "alexnet_model_accuracy = evaluate_model(alexnet_model, test_dataloader)\n",
        "\n",
        "# Load the pre-trained DenseNet model\n",
        "dn_model_path = \"C:\\\\Users\\\\addep\\\\OneDrive\\\\Documents\\\\Densenet.pth\"\n",
        "dn_model = models.densenet201(pretrained=True)\n",
        "dn_model.classifier = nn.Linear(dn_model.classifier.in_features, len(test_dataset.classes))\n",
        "dn_model.load_state_dict(torch.load(dn_model_path), strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dn_model.to(device)\n",
        "dn_model.eval()\n",
        "\n",
        "# Evaluate the DenseNet model on the test dataset\n",
        "dn_model_accuracy = evaluate_model(dn_model, test_dataloader)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy for AlexNet Model: {alexnet_model_accuracy:.2f}%\")\n",
        "print(f\"Accuracy for DenseNet Model: {dn_model_accuracy:.2f}%\")\n",
        "\n",
        "# Comparative graph\n",
        "models = ['AlexNet', 'DenseNet']\n",
        "accuracies = [alexnet_model_accuracy, dn_model_accuracy]\n",
        "\n",
        "plt.bar(models, accuracies, color=['blue', 'green'])\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Comparison of AlexNet and DenseNet on Test Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "df_S-8Sig6Xk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}